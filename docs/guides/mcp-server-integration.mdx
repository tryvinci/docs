---
title: "Using Vinci's MCP Tools with LangChain Agents"
description: "A cookbook for integrating Vinci's MCP tools into your own LangChain agentic projects."
---

# Using Vinci's MCP Tools with LangChain Agents

This document provides a developer-focused guide on how to integrate Vinci's MCP server with your own LangChain agent. It's structured as a cookbook with recipes to guide you through the implementation.

## Overview

This guide shows how you can connect your LangChain agent to Vinci's MCP (Multi-tool Control Plane) server. This enables your agent to dynamically discover and utilize our powerful external tools, extending its capabilities beyond simple chat.

## Prerequisites

Before you begin, make sure you have the necessary packages installed.

```bash
pip install langchain langchain-google-genai fastmcp python-dotenv
```

## Configuration

To run the examples in this guide, you need to configure the following environment variables in your `.env` file.

```env
# .env

# The URL of Vinci's MCP server
MCP_SERVER_URL="https://leonardo-serving-382403086889.asia-south1.run.app/agent/mcp"

# Your Google API key for Gemini
GEMINI_API_KEY="your_gemini_api_key"

# Your Vinci User ID
USER_ID="your_user_id"
```

<Note>
  You can find your `USER_ID` on the [Vinci Dashboard API page](https://app.tryvinci.com/dashboard/api).
</Note>

---

## Recipes

Here are some recipes to demonstrate how to work with the MCP server integration.

### Recipe 1: Fetching Tools from the MCP Server

This recipe shows how to fetch the available tools from the MCP server and convert them into LangChain `StructuredTool` objects. This is the first step to making the tools available to the agent.

The primary function for this is `get_mcp_tools` in `agents/utils.py`.

**Code:**

```python
# tools/mcp_utils.py

from typing import Callable, Coroutine, Any
from fastmcp.client import Client
from fastmcp.model import Tool

def create_coro(mcp_client: Client, tool: Tool) -> Callable[..., Coroutine[Any, Any, Any]]:
    """
    Creates a coroutine that executes a tool on the MCP server.
    
    This factory function is necessary to properly capture the `tool` variable
    in a closure for each iteration of the tool creation loop.
    """
    async def coro(*args, **kwargs):
        return await mcp_client.execute(tool.name, {**kwargs})
    return coro
```

```python
# agents/utils.py

import os
import logging
from typing import List
from fastmcp import Client
from fastmcp.client.transports import StreamableHttpTransport
from langchain.tools import StructuredTool
from tools.mcp_utils import create_coro

logger = logging.getLogger(__name__)

async def get_mcp_tools() -> List[StructuredTool]:
    """
    Initializes the FastMCP client and fetches the available tools,
    converting them to LangChain StructuredTools.
    """
    server_url = os.getenv("MCP_SERVER_URL")
    user_id = os.getenv("USER_ID")

    if not server_url or not user_id:
        logger.warning("MCP_SERVER_URL or USER_ID not set. No tools will be loaded.")
        return []

    headers = {"X-User-ID": user_id}
    transport = StreamableHttpTransport(server_url, headers=headers)
    mcp_client = Client(transport)
    
    try:
        async with mcp_client:
            raw_tools = await mcp_client.list_tools()
            
        structured_tools = [
            StructuredTool.from_function(
                coroutine=create_coro(mcp_client, tool),
                name=tool.name,
                description=tool.description,
                args_schema=tool.inputSchema,
            )
            for tool in raw_tools
        ]
        logger.info(f"Successfully loaded {len(structured_tools)} tools from MCP server.")
        return structured_tools
    except Exception as e:
        logger.error(f"Could not set up MCP tools: {e}")
        return []
```

### Recipe 2: Integrating Tools with Your LangChain Agent

Once you have a way to fetch tools, you can integrate them into your LangChain agent. Here is a simple example of how to create an agent that uses the MCP tools.

**Code:**

```python
# your_agent_setup.py

import os
import asyncio
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain import hub
from agents.utils import get_mcp_tools

# Your model name and max iterations
MODEL_NAME = "gemini-2.5-flash"
MAX_ITERATIONS = 10

# Pull the prompt from LangChain Hub
AGENT_PROMPT_HUB = "hwchase17/structured-chat-agent"
prompt = hub.pull(AGENT_PROMPT_HUB)

async def create_agent_executor():
    """
    Creates a LangChain agent executor with tools from the MCP server.
    """
    # Fetch tools from the MCP server
    mcp_tools = await get_mcp_tools()

    # Initialize your LLM
    llm = ChatGoogleGenerativeAI(
        model=MODEL_NAME,
        google_api_key=os.getenv("GEMINI_API_KEY"),
        temperature=0.7,
    )

    # Create the agent
    agent = create_structured_chat_agent(llm, mcp_tools, prompt)

    # Create the agent executor
    executor = AgentExecutor(
        agent=agent,
        tools=mcp_tools,
        verbose=True,
        handle_parsing_errors=True,
        max_iterations=MAX_ITERATIONS,
    )

    return executor

async def main():
    """
    Example of how to use the agent executor.
    """
    agent_executor = await create_agent_executor()
    
    # Now you can run the agent
    result = await agent_executor.ainvoke({"input": "List all the tools you have access to."})
    print(result)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## Conclusion

By following the recipes in this guide, you can seamlessly integrate Vinci's powerful MCP tools into your own LangChain agents. This allows you to build sophisticated, tool-augmented applications with ease. We encourage you to experiment with the available tools and explore the creative possibilities they unlock.
